{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 05, ASTR 496: Foundations of Data Science in Astronomy\n",
    "\n",
    "\n",
    "## Sampling and MCMC\n",
    "\n",
    "### Gautham Narayan \n",
    "##### <gsn@illinois.edu>\n",
    "\n",
    "Borrowing heavily from David Kirkby and Adam Mantz this week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Markov Chains\n",
    "\n",
    "* We've come up with a model, an objective/loss/likelihood function and some priors\n",
    "* Now we actually want to evaluate the posterior $P(\\theta|D)$\n",
    "    * analytically is too hard, so we draw samples (i.e. **Monte Carlo**) \n",
    "        * convert messy integrals to sums over the samples \n",
    "* Simple Monte Carlo was wasteful because we don't want to draw samples over all parameter space\n",
    "    * one way to make it more efficient was make samples that are correlated with each other\n",
    "        * keep sampling in regions of high probability, don't sample in regions without\n",
    "        * Markov Chains are series of samples where every sample only depends on the previous one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Want to setup the problem so the frequency of samples in some region of width $dx$ **is proportional** to some function of $x$\n",
    "* There are two ways to do this:\n",
    "\n",
    "    * **Inverse Transform sampling** - too restrictive, because functions must be invertible\n",
    "    * Rejection sampling - general purpose\n",
    "* If we're using rejection sampling to pick samples on our Markov Chain, we're doing **Markov Chain Monte Carlo** with **Metropolis Hastings** as our sampling strategy\n",
    "    * The **equilibrium** state of this chain is the stationary distribution of samples we desire\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In-class exercise 1: Inverse Transform Sampling\n",
    "\n",
    "We deal with exponential distributions a lot in astrophysics. \n",
    "\n",
    "This distribution has $p(x)=\\lambda e^{-\\lambda x}$ and $F(x)=1-e^{-\\lambda x}$ for $x\\geq0$.\n",
    "\n",
    "The quantile function is, therefore, $F^{-1}(P) = -\\ln(1-P)/\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca69c9765ae4a4ca219da303348bf8f",
       "version_major": 2,
       "version_minor": 0
      },
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAAHgCAYAAAA10dzkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAIyUlEQVR4nO3WMQEAIAzAMMC/5+ECjiYKenbPzCwAADLO7wAAAN4ygAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIuJnkHvKensmIAAAAASUVORK5CYII=",
      "text/html": [
       "\n",
       "            <div style=\"display: inline-block;\">\n",
       "                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n",
       "                    Figure\n",
       "                </div>\n",
       "                <img src='data:image/png;base64,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' width=640.0/>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here's some code for the inverse tranform\n",
    "# it accepts a vector x, that is hopefully exponentially distributed and the argument for an exponential lam\n",
    "# it then plots the vector and the distribution\n",
    "# RUN THIS\n",
    "\n",
    "plt.figure()\n",
    "def inv_trans(x, lam):\n",
    "    hist = plt.hist(x, bins=50, density=True)\n",
    "    xs = np.linspace(0.0, 10.0/lam, 100)\n",
    "    pdf = lam * np.exp(-lam*xs)\n",
    "    pdfline = plt.plot(xs, pdf, 'r', lw=2)\n",
    "    plt.xlabel(r'x', fontsize=22)\n",
    "    plt.ylabel(r'P(x)', fontsize=22);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# for lam = 2., draw a 10000 random samples from a ** unform distribution **\n",
    "# and use inverse CDF to convert to samples from a exponential distribution\n",
    "# then use the inv_trans code to histogram your samples and overplot the distribution.\n",
    "s = np.random.uniform(0, 1, 10000)\n",
    "lam = 2.\n",
    "x = -np.log(1-s)/lam\n",
    "inv_trans(x, lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In-class exercise 2: A Full MCMC example with affine-invariant MC (i.e. `emcee`)\n",
    "\n",
    "* Based on the paper by [Goodman and Weare](https://msp.org/camcos/2010/5-1/p04.xhtml)\n",
    "\n",
    "* implemented in `emcee`\n",
    "\n",
    "* many walkers simultaneously generating correlated Markov Chains\n",
    "\n",
    "* **Affine Invariance** - efficiency not impacted by any linear (aka affine) transformation of the parameter space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is one of a class of methods that evolve an *ensemble* of states rather than a single state.\n",
    "After convergence, the ensemble can be regarded as a set of samples from the target distribution.\n",
    "\n",
    "This approach provides some of the benefits of running multiple chains - but remember that these are not *independent* chains!\n",
    "\n",
    "### The Goodman-Weare method\n",
    "\n",
    "The algorithm for moving each point in the ensemble is:\n",
    "1. Randomly pick a different point from the ensemble (total size $N$).\n",
    "2. Propose a move in the direction of that point, by the distance between them multiplied by a random from this distribution:\n",
    "$g(z) \\propto \\frac{1}{\\sqrt z}; ~ \\frac{1}{2}\\leq z \\leq 2$\n",
    "3. Accept or reject the move based on the ratio of posterior densities multiplied by $z^{N-1}$.\n",
    "\n",
    "Note that there is some magic in the density $g$. We are not free to choose just any function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This algorithm is relatively easy to use - there is no tuning required and it's straightforward to parallelize.\n",
    "\n",
    "Important cautions:\n",
    "* if the ensemble is not started in a region of high probability, convergence will be **extremely** slow. Y\n",
    "* as the walkers are not independent, the Gelman-Rubin convergence criterion doesn't apply\n",
    "* assessing convergence visually is not always straightforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# In class exercise - Using `emcee` to deal with the Hogg data problem\n",
    "\n",
    "Same idea as Hogg data in the homework - we have some x, y data with outliers, but rather than use the Huber loss (which was a bit ad hoc)\n",
    "we'll define a mixture likelihood and then use affine-invariant MCMC to sample the posterior. Critically, this will not have a tuning step! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'emcee'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# RUN THIS\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01memcee\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcorner\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimize\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'emcee'"
     ]
    }
   ],
   "source": [
    "# RUN THIS\n",
    "\n",
    "\n",
    "import emcee\n",
    "import corner\n",
    "from scipy import optimize\n",
    "\n",
    "# we'll create x, y, error values\n",
    "x = np.array([ 0,  3,  9, 14, 15, 19, 20, 21, 30, 35,\n",
    "              40, 41, 42, 43, 54, 56, 67, 69, 72, 88])\n",
    "y = np.array([33, 68, 34, 34, 37, 71, 37, 44, 48, 49,\n",
    "              53, 49, 50, 48, 56, 60, 61, 63, 44, 71])\n",
    "e = np.array([ 3.6, 3.9, 2.6, 3.4, 3.8, 3.8, 2.2, 2.1, 2.3, 3.8,\n",
    "               2.2, 2.8, 3.9, 3.1, 3.4, 2.6, 3.4, 3.7, 2.0, 3.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# RUN THIS\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# just as before, we can define a simple chisquared function\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msquared_loss\u001b[39m(theta, x\u001b[38;5;241m=\u001b[39mx, y\u001b[38;5;241m=\u001b[39my, e\u001b[38;5;241m=\u001b[39me):\n\u001b[1;32m      6\u001b[0m     dy \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m-\u001b[39m theta[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m theta[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m x\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (dy \u001b[38;5;241m/\u001b[39m e) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "# RUN THIS\n",
    "\n",
    "# just as before, we can define a simple chisquared function\n",
    "\n",
    "def squared_loss(theta, x=x, y=y, e=e):\n",
    "    dy = y - theta[0] - theta[1] * x\n",
    "    return np.sum(0.5 * (dy / e) ** 2)\n",
    "\n",
    "theta1 = optimize.fmin(squared_loss, [0, 0], disp=False)\n",
    "plt.figure()\n",
    "xfit = np.linspace(0, 100)\n",
    "plt.errorbar(x, y, e, fmt='.k', ecolor='gray')\n",
    "plt.plot(xfit, theta1[0] + theta1[1] * xfit, '-k')\n",
    "plt.title('Maximum Likelihood fit: Squared Loss');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhuber_loss\u001b[39m(t, c\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ((\u001b[38;5;28mabs\u001b[39m(t) \u001b[38;5;241m<\u001b[39m c) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m t \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mabs\u001b[39m(t) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m c) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39mc \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m c \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mabs\u001b[39m(t)))\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtotal_huber_loss\u001b[39m(theta, x\u001b[38;5;241m=\u001b[39mx, y\u001b[38;5;241m=\u001b[39my, e\u001b[38;5;241m=\u001b[39me, c\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m huber_loss((y \u001b[38;5;241m-\u001b[39m theta[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m theta[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m x) \u001b[38;5;241m/\u001b[39m e, c)\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     12\u001b[0m theta2 \u001b[38;5;241m=\u001b[39m optimize\u001b[38;5;241m.\u001b[39mfmin(total_huber_loss, [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], disp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "# RUN THIS\n",
    "\n",
    "# we can also make a Huber loss function as before and get a fit that is more resilient to outliers\n",
    "\n",
    "def huber_loss(t, c=3):\n",
    "    return ((abs(t) < c) * 0.5 * t ** 2\n",
    "            + (abs(t) >= c) * -c * (0.5 * c - abs(t)))\n",
    "\n",
    "def total_huber_loss(theta, x=x, y=y, e=e, c=3):\n",
    "    return huber_loss((y - theta[0] - theta[1] * x) / e, c).sum()\n",
    "\n",
    "theta2 = optimize.fmin(total_huber_loss, [0, 0], disp=False)\n",
    "plt.figure()\n",
    "plt.errorbar(x, y, e, fmt='.k', ecolor='gray')\n",
    "plt.plot(xfit, theta1[0] + theta1[1] * xfit, color='red')\n",
    "plt.plot(xfit, theta2[0] + theta2[1] * xfit, color='black')\n",
    "plt.title('Maximum Likelihood fit: Huber loss');\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Bayesian approach to accounting for outliers generally involves *modifying the model* so that the outliers are accounted for. For this data, it is abundantly clear that a simple straight line is not a good fit to our data. So let's propose a more complicated model that has the flexibility to account for outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One option is to choose a mixture between a signal and a background:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "p(\\{x_i\\}, \\{y_i\\},\\{e_i\\}~|~\\theta,\\{g_i\\},\\sigma,\\sigma_b) = & \\frac{g_i}{\\sqrt{2\\pi e_i^2}}\\exp\\left[\\frac{-\\left(\\hat{y}(x_i~|~\\theta) - y_i\\right)^2}{2e_i^2}\\right] \\\\\n",
    "&+ \\frac{1 - g_i}{\\sqrt{2\\pi \\sigma_B^2}}\\exp\\left[\\frac{-\\left(\\hat{y}(x_i~|~\\theta) - y_i\\right)^2}{2\\sigma_B^2}\\right]\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "What we've done is expanded our model with some nuisance parameters: $\\{g_i\\}$ is a series of weights which range from 0 to 1 and encode for each point $i$ the degree to which it fits the model. $g_i=0$ indicates an outlier, in which case a Gaussian of width $\\sigma_B$ is used in the computation of the likelihood. This $\\sigma_B$ can also be a nuisance parameter, or its value can be set at a sufficiently high number, say 50.\n",
    "\n",
    "This model is a mixture of two Gaussians and is something we'll use a lot when we have multiple underlying populations that we want to model at the same time. We generally call these Gaussian mixture models, because we're really super creative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our model is much more complicated now: it has 22 parameters rather than 2, but the majority of these can be considered nuisance parameters, which can be marginalized-out in the end, just as we marginalized (integrated) over $p$ in the Billiard example.  Let's construct a function which implements this likelihood. As in the previous post, we'll use the [emcee](https://emcee.readthedocs.io/en/stable/) package to explore the parameter space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To actually compute this, we'll start by:\n",
    "* defining the model (done implicitly) \n",
    "* functions describing our likelihood function (done above) \n",
    "* our prior (we'll make this up)\n",
    "* and posterior (defined once we have likelihood and prior)\n",
    "\n",
    "And then you get to use `emcee` to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# DO THIS\n",
    "\n",
    "# A. first you need to define a prior \n",
    "# there is a basic choice for prior ( i.e. g_i has to be between 0 and 1)\n",
    "# we'll store the g_i in theta[2:] \n",
    "# your prior should be to check if 0 < theta[2:] < 1 - if it is, return 0. If not return -inf\n",
    "\n",
    "def log_prior(theta):\n",
    "    #g_i needs to be between 0 and 1\n",
    "    ### YOUR CODE GOES HERE ####\n",
    "    \n",
    "    if all(theta[2:] > 0) and all(theta[2:] < 1):\n",
    "        return 0\n",
    "    else:\n",
    "        return -np.inf\n",
    " \n",
    "# B. Then you need to define a likelihood function\n",
    "# the likelihood function is the same as we write above, except now as a log\n",
    "def log_likelihood(theta, x, y, e, sigma_B):\n",
    "    \n",
    "    # this is the residual\n",
    "    dy = y - theta[0] - theta[1] * x\n",
    "    \n",
    "    # this is again forcing the g_i to be within 0 and 1\n",
    "    g = np.clip(theta[2:], 0, 1)  # g<0 or g>1 leads to NaNs in logarithm\n",
    "    \n",
    "    # you should write two log-likelihood terms to implement the sum in the above formula\n",
    "    # as logL1 and logL2\n",
    "    \n",
    "    ### YOUR CODE GOES HERE ####\n",
    "    logL1 = (g / np.sqrt(2 * np.pi * e**2)) * np.exp(-(y - dy)**2 / (2*e**2))\n",
    "    logL2 = ((1-g) / np.sqrt(2 * np.pi * sigma_B**2)) * np.exp(-(y - dy)**2 / (2*sigma_B**2))\n",
    "    \n",
    "    # we have the sum of two PDFs that we took the log of\n",
    "    # to do this when we write those PDFs itself as logs, we can use np.logaddexp\n",
    "    return np.sum(np.logaddexp(logL1, logL2))\n",
    "\n",
    "\n",
    "# C. Finally, you need to define the posterior\n",
    "def log_posterior(theta, x, y, e, sigma_B):\n",
    "    # posterior is product of likelihood and prior (or sum of log likelihoods)\n",
    "    \n",
    "    ### YOUR CODE GOES HERE ###\n",
    "    return log_likelihood(theta, x, y, e, sigma_B) + log_prior(theta)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'theta1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     18\u001b[0m starting_guesses \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((nwalkers, ndim))\n\u001b[0;32m---> 19\u001b[0m starting_guesses[:, :\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(theta1, \u001b[38;5;241m1\u001b[39m, (nwalkers, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     20\u001b[0m starting_guesses[:, \u001b[38;5;241m2\u001b[39m:] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, (nwalkers, ndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# D. RUN THE MCMC\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# look at the emcee documentation for EnsembleSampler to create a sampler object \u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# and the run_mcmc() function\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m####  YOUR CODE GOES HERE ####\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'theta1' is not defined"
     ]
    }
   ],
   "source": [
    "# I'll do the other setup for you\n",
    "# Note that this step will take a few seconds to run!\n",
    "\n",
    "ndim = 2 + len(x)  # number of parameters in the model\n",
    "nwalkers = 50  # number of MCMC walkers\n",
    "\n",
    "# Burnin\n",
    "# initially the Markov chain is clearly not in it's stationary state at all \n",
    "# Conservatively, we might remove the first ~5000 steps based on this. \n",
    "# This period is called the **burn in**.\n",
    "\n",
    "nburn = 5000  # \"burn-in\" period to let chains stabilize\n",
    "nsteps = 15000  # number of MCMC steps to take\n",
    "\n",
    "# set theta near the maximum likelihood, with \n",
    "# theta1 is from the mean squared from earlier\n",
    "np.random.seed(10)\n",
    "starting_guesses = np.zeros((nwalkers, ndim))\n",
    "starting_guesses[:, :2] = np.random.normal(theta1, 1, (nwalkers, 2))\n",
    "starting_guesses[:, 2:] = np.random.normal(0.5, 0.1, (nwalkers, ndim - 2))\n",
    "\n",
    "\n",
    "# D. RUN THE MCMC\n",
    "# look at the emcee documentation for EnsembleSampler to create a sampler object \n",
    "# and the run_mcmc() function\n",
    "####  YOUR CODE GOES HERE ####\n",
    "emcee.EnsembleSampler(nwalkers, ndim, log_posterior, args=(x, y, e, 50)).run_mcmc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sampler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample \u001b[38;5;241m=\u001b[39m sampler\u001b[38;5;241m.\u001b[39mchain  \u001b[38;5;66;03m# shape = (nwalkers, nsteps, ndim)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sample \u001b[38;5;241m=\u001b[39m sampler\u001b[38;5;241m.\u001b[39mchain[:, nburn:, :]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, ndim)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sampler' is not defined"
     ]
    }
   ],
   "source": [
    "sample = sampler.chain  # shape = (nwalkers, nsteps, ndim)\n",
    "sample = sampler.chain[:, nburn:, :].reshape(-1, ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# the shape here is (nsteps - nburn)* nwalkers, ndim\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once we have these samples, we can exploit a very nice property of the Markov chains. Because their distribution models the posterior, we can integrate out (i.e. marginalize) over nuisance parameters simply by ignoring them!\n",
    "\n",
    "We can look at the (marginalized) distribution of slopes and intercepts by examining the first two columns of the sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(sample[:, 0], sample[:, 1], ',k', alpha=0.1)\n",
    "plt.xlabel('intercept')\n",
    "plt.ylabel('slope');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We see a distribution of points near a slope of $\\sim 0.45$, and an intercept of $\\sim 31$. We'll plot this model over the data below, but first let's see what other information we can extract from this trace.\n",
    "\n",
    "One nice feature of analyzing MCMC samples is that the choice of nuisance parameters is completely symmetric: just as we can treat the $\\{g_i\\}$ as nuisance parameters, we can also treat the slope and intercept as nuisance parameters! Let's do this, and check the posterior for $g_1$ and $g_2$, the outlier flag for the first two points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(sample[:, 2], sample[:, 3], ',k', alpha=0.1)\n",
    "plt.xlabel('$g_1$')\n",
    "plt.ylabel('$g_2$')\n",
    "plt.show()\n",
    "\n",
    "print(\"g1 mean: {0:.2f}\".format(sample[:, 2].mean()))\n",
    "print(\"g2 mean: {0:.2f}\".format(sample[:, 3].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There is not an extremely strong constraint on either of these, but we do see that $(g_1, g_2) = (1, 0)$ is slightly favored: the means of $g_1$ and $g_2$ are greater than and less than 0.5, respecively. If we choose a cutoff at $g=0.5$, our algorithm has identified $g_2$ as an outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's make use of all this information, and plot the marginalized best model over the original data. As a bonus, we'll draw red circles to indicate which points the model detects as outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "theta3 = np.mean(sample[:, :2], 0)\n",
    "g = np.mean(sample[:, 2:], 0)\n",
    "outliers = (g < 0.5)\n",
    "plt.figure()\n",
    "plt.errorbar(x, y, e, fmt='.k', ecolor='gray')\n",
    "plt.plot(xfit, theta1[0] + theta1[1] * xfit, color='lightgray')\n",
    "plt.plot(xfit, theta2[0] + theta2[1] * xfit, color='lightgray')\n",
    "plt.plot(xfit, theta3[0] + theta3[1] * xfit, color='black')\n",
    "plt.plot(x[outliers], y[outliers], 'ro', ms=20, mfc='none', mec='red')\n",
    "plt.title('Maximum Likelihood fit: Bayesian Marginalization');\n",
    "plt.show()\n",
    "\n",
    "# you can even use the samples to build a confidence interval for the fit!\n",
    "# here we will sample the slopes and intercepts randomly\n",
    "inds = np.random.choice(np.arange(0, len(sample[:, 0])), 30)\n",
    "intercepts = sample[:,0][inds]\n",
    "slopes = sample[:,1][inds]\n",
    "# calculate lines for each of the slopes and intercepts\n",
    "y_vals = np.add( np.outer(slopes, xfit), intercepts[:,np.newaxis])\n",
    "# and determine the standard deviation of our estimate at each location\n",
    "y_stds = np.std(y_vals, axis=0)\n",
    "plt.fill_between(xfit, theta3[0] + theta3[1] * xfit - y_stds, theta3[0] + theta3[1] * xfit + y_stds, color='dodgerblue', alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
